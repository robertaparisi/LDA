{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installiamo i due pacchetti\n",
    "Da far eseguire solo la prima volta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install msgpack\n",
    "!pip install pyspark==2.3.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importiamo le librerie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = \"--master local[8] pyspark-shell\"\n",
    "os.environ[\"JAVA_HOME\"] = \"C:\\jdk1.8.0_171\"\n",
    "#os.environ[\"JAVA_HOME\"] = \"C:\\Program Files\\Java\\jdk-10.0.1\"\n",
    "\n",
    "\n",
    "#USATO PER RISOLVEREE PROBLEMA JAVA GATEWAY ..... (ho anche installato java e inserito manualmente java home tra le variabili ambiente)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\paperspace\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer, CountVectorizer\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.linalg import Vectors, SparseVector\n",
    "from pyspark.ml.clustering import LDA, BisectingKMeans\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import pyspark\n",
    "import string\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.mllib.util import MLUtils\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.feature import CountVectorizer, CountVectorizerModel, Tokenizer, RegexTokenizer, StopWordsRemover\n",
    "nltk.download(\"stopwords\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creiamo la sessione di spark e importiamo il dataset da un documento json\n",
    "\n",
    "Il campo di interesse è testo_txt_it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('NLP_topicModel').getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+--------+----------+--------------------+--------------------+--------------------+---+----------+\n",
      "|          _version_|                  id|language|language_s|        published_dt|        testo_txt_it|               url_s|uid|year_month|\n",
      "+-------------------+--------------------+--------+----------+--------------------+--------------------+--------------------+---+----------+\n",
      "|1600612814188183552|2cfcaaac52cfbb66d...|        |   italian|2018-05-15T14:08:00Z|| 0\n",
      "Dal 1° al 10 ...|http://omgili.com...|  0|   2018-05|\n",
      "|1600612814388461568|0cf575e54190bff2e...|        |   italian|2018-05-15T14:08:00Z|15 maggio 2018 16...|http://omgili.com...|  1|   2018-05|\n",
      "|1600612814394753024|b8dbc4e7826b41575...|        |   italian|2018-05-15T14:08:00Z|Leonardo Nappi 13...|http://omgili.com...|  2|   2018-05|\n",
      "+-------------------+--------------------+--------+----------+--------------------+--------------------+--------------------+---+----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rawdata = spark.read.json(\"ARTICOLITV2.json\", multiLine=True)\n",
    "rawdata = rawdata.fillna({'testo_txt_it': ''})\n",
    "rawdata = rawdata.withColumn(\"uid\", monotonically_increasing_id())\n",
    "rawdata = rawdata.withColumn(\"year_month\", rawdata.published_dt.substr(1,7))\n",
    "rawdata.show(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def getrows(df, rownums=None):\n",
    "#     return df.rdd.zipWithIndex().filter(lambda x: x[1] in rownums).map(lambda x: x[0])\n",
    "#getrows(rawdata.select(\"testo_txt_it\"), [2]).collect()\n",
    "#rawdata.select(\"testo_txt_it\").rdd.zipWithIndex().filter(lambda x: x[1] in [2]).map(lambda x: x[0])\n",
    "#rawdata.select(\"testo_txt_it\").show(3, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sc = SparkContext.getOrCreate()\n",
    "#sc._conf.getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rawdata.select(\"testo_txt_it\").show(3, False)#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Puliamo i dati:\n",
    "- Rimuoviamo la punteggiatura\n",
    "- Applichiamo la tokenization per avere singole parole\n",
    "- Applichiamo lo stemming al nostro dataset\n",
    "- Rimuoviamo le stopwords:\n",
    "    - Rimuoviamo le parole che sono composte da meno di 3 lettere\n",
    "    - Rimuoviamo le parole che capitano frequentemente nei nostri documenti\n",
    "    - Rimuoviamo le stopwords del pacchetto NLTK (italiane e inglesi)\n",
    "    - Rimuoviamo le stopwords di PySpark \n",
    "    - Rimuoviamo tutte queste stopwords applicandogli lo stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myStemmer (record, array=False):\n",
    "    stemmer = SnowballStemmer(\"italian\")\n",
    "    if array:\n",
    "        text_out = [stemmer.stem(word) for word in record] \n",
    "    else:    \n",
    "        text = record [2]#9\n",
    "        text_out = [stemmer.stem(word) for word in text]\n",
    "    return(text_out)\n",
    "\n",
    "    \n",
    "\n",
    "#def myCleaner (record):\n",
    "#    text = record [10]\n",
    "#    text_out = [re.sub(\"[^a-zA-Z]\",\"\", word.strip().lower()) for word in text]\n",
    "#    text_out = [word for word in text if (word != \"\" and word != \" \")]\n",
    "#    return(text_out)\n",
    "\n",
    "def removePunctuation(column):\n",
    "    return trim(lower(regexp_replace(column, '[^A-Za-z,]', ' '))).alias('sentence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Contains digits\",contains_digits)\n",
    "# print(\"more_then_3_charachters\",more_then_3_charachters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prova = rawdata.select(removePunctuation(col(\"testo_txt_it\")))\n",
    "#Tokenize the text in the text column\n",
    "tokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"tokenized\")#testo_txt_it\n",
    "wordsDataFrame = tokenizer.transform(prova)#rawdata\n",
    "\n",
    "remover = StopWordsRemover(inputCol=\"tokenized\", outputCol=\"tokenizedNew\", stopWords = [\" \", \"\"])\n",
    "wordsDataFrame = remover.transform(wordsDataFrame)\n",
    "\n",
    "udf_myStemmer = udf(myStemmer, (ArrayType(StringType()))) # if the function returns an int\n",
    "wordsDataFrame = wordsDataFrame.withColumn(\"stemmed\", udf_myStemmer(struct([wordsDataFrame[x] for x in wordsDataFrame.columns])))\n",
    "\n",
    "# udf_myFunction = udf(myCleaner, (ArrayType(StringType()))) # if the function returns an int\n",
    "# wordsDataFrame = wordsDataFrame.withColumn(\"cleaned\", udf_myFunction(struct([wordsDataFrame[x] for x in wordsDataFrame.columns])))\n",
    "\n",
    "# #remove 20 most occuring documents, documents with non numeric characters, and documents with <= 3 characters\n",
    "cv_tmp = CountVectorizer(inputCol=\"stemmed\", outputCol=\"tmp_vectors\")\n",
    "cv_tmp_model = cv_tmp.fit(wordsDataFrame)\n",
    "\n",
    "#wordsDataFrame.show(5)\n",
    "\n",
    "\n",
    "\n",
    "top100 = list(cv_tmp_model.vocabulary[0:100])\n",
    "stopWordsCustom = [\" \",\"\", \"dal\", \"lung\", \"tant\", \"al\",\"davan\",\"avev\",\"qualc\", \"qualcuno\", \"qualcosa\", \"avevano\", \"davanti\", \"aveva\",\"e\",\"avere\", \"fare\",\"la\",\"li\", \"lo\", \"gli\", \"essere\", \"solo\", \"per\", \"cosa\", \"ieri\",\"disponibile\", \"anno\", \"detto\", \"quando\",\"fatto\", \"sotto\", \"alcuna\", \"quali\"]\n",
    "#Add additional stopwords in th, is list\n",
    "\n",
    "more_then_3_charachters = [word for word in cv_tmp_model.vocabulary if len(word) <= 3 ]\n",
    "stopWords = list(set(stopwords.words('english')))+list(set(stopwords.words('italian')))\n",
    "#Combine all the stopwords\n",
    "stpw = StopWordsRemover.loadDefaultStopWords(\"italian\") + stopWords+ stopWordsCustom +top100 + more_then_3_charachters \n",
    "stem_stopw = myStemmer(stpw, True) #stemming the stopwords\n",
    "\n",
    "#Remove stopwords from the tokenized list\n",
    "removerNew = StopWordsRemover(inputCol=\"stemmed\", outputCol=\"final\", stopWords =   stem_stopw+stpw) \n",
    "clean_text= removerNew.transform(wordsDataFrame)#dropping the stemmed stopwords from the stemmed word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|             Stemmed|\n",
      "+--------------------+\n",
      "|[dal, al, giugn, ...|\n",
      "|[magg, condivisio...|\n",
      "|[leonard, napp, m...|\n",
      "|[twitter, exop, o...|\n",
      "|[mir, debutt, av,...|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wordsDataFrame.select(\"Stemmed\").show(5) #parole stemmate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|final                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|[prestig, cornic, palazz, recuper, cutore,, cuor, bonaccors, ospiter, etna, phot, meeting,, festival, fotograf, organizz, grupp, fotograf, valverd, giunt, esim, edizion, scelt, mott, latin, libr, faciunt, labr, libr, parol, centr, manifestazione,, infatti,, legam, profond, intercorr, fotograf, libri,, letteratura,, raccont, luoghi,, memorie,, sensazion, calendar, diec, eventi,, mostre,, libri,, workshop,, lettur, portfol, estemporane, fotograf, ospit, onore,, fotograf, ferdin, sciann, insign, prem, sicil, bagheria,, fotograf, entrar, prestig, agenz, internazional, magnum, photos, invit, henr, cartier, bresson, carrier, costell, amiciz, collabor, scrittor, successo,, leonard, sciasc, manuel, zquez, montalb, jorg, luis, borges, sciann, realizzato,, altre, cose,, reportag, fest, relig, siciliane,, ritratt, premio,, istitu, rend, omagg, fotograf, distint, panoram, internazionale,, assegn, personagg, giusepp, leon, piergiorg, branz, miglior, mauriz, galimb, francesc, meris, giulian, travers, francesc, radin, colomb, domen, ruiu, oliv, barbier, cerimon, consegn, premio,, previst, venerd, inaugurer, diec, event, dedic, mond, fotografia,, mostr, autor, sicilian, angel, pitrone,, martin, zummo,, robert, stran, valer, laudan, dedic, sicil, workshop, stamp, professional, anton, mant, arte, ritratt, antonell, cunsol, ancora,, parler, libr, fotograf, ferdin, sciann, presenter, dolor, viss, farfall, best, animal, contrast, appen, uscit, librer, editr, contrast, appunt, attes, lettur, portfol, portfol, insiem, esim, edizion, avvarr, presenz, lettor, eccezion, triestin, fulv, merlak,, president, onor, fiaf, feder, italian, assoc, fotograf, calabres, attil, lauria,, president, fiaf, complet, programm, estemporane, fotograf, edizion, concors, possibilit, mett, prov, talent, conferenze,, avvoc, pipp, pappalardo,, altra, fulv, merlak, inerent, arte, fotograf, kermess, concluder, domen, tradizional, dedic, audiovis, fotograf, dipart, audiovis, feder, italian, assoc, fotograf, programm, complet, manifest, scaric, fotoclublegru, info, contatt, grupp, fotograf, valverd, info, fotoclublegru, cell, ferdin, portues, cell, seren, vast]|\n",
      "|[magg, condivision, magg, guid, che,, centr, storic, arriv, piazz, miracol, porter, scopert, figur, galile, galile, attravers, aneddoti,, personal, colleg, citt, natale,, partecip, potrann, luog, scienz, info, prenot, obbligator, info, impegnoefutur, cost, ritrov, previst, piazz, garibald, argoment]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |\n",
      "|[leonard, napp, magg, santuar, lourdes, mattin]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |\n",
      "|[twitter, exop, offre, serviz, analis, risc, combin, intelligt, uman, machin, learning, potenzialit, tecnolog, ambit, analis, linguagg, natural, riusc, interpret, document, font, apert, stim, pericol, paese,, special, viaggiator, cont, imprese,, ambit, azion, exop, incontr, occasion, iothings, matte, grell, head, researc, exop, specializz, soluzion, analis, rischio,, paese,, ogni, punt, vist, particol, matte, grell, segu, process, trasform, digitalizz, ambit, sfid, applic, tecnolog, machin, learning, automatizz, process, altriment, appannagg, risors, uman, emul, livell, macroscop, process, cognit, person, quind, obiett, exop, ricr, semplific, determin, processi,, analis, automat, inform, occup, matte, grell, signif, pratic, abilit, macchin, comprend, autonom, conten, espress, document, scritt, matte, grell, attent, utilizz, parol, spieg, come,, oggi,, utilizz, macrotermin, artificial, intelligenc, indic, ambito,, emul, process, cognit, uman, intern, sfer, applic, tecnolog, machin, learning, assimil, parzial, concret, deep, learning, sfrutt, analis, natural, languag, processing, machin, learning, implic, utilizz, neurali,, sistem, artificial, emul, modell, biolog, specif, exop, propon, piattaform, gestion, risc, mobilit, complet, automatizz, monitor, automat, risc, viaggiator, aziendali,, mond, esigt, specif, client, includ, monitoragg, global, eventi,, analis, risc, livell, paes, citt, monitoragg, posizioni,, elev, livell, automatizz, regol, individual, solo,, soluzion, exop, shortcut, poss, offrir, report, valut, risc, impres, paes, mondo,, minim, impegn, econom, client, vide, domand, rispost, matte, grella,, head, researc, exop, condivid, articol]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |\n",
      "|[debutt, experienc, conclus, marted, scors, terz, edizion, rimin, comunic, stamp, parl, partecip, circ, presenz, disposizion, ampi, scelt, attivit, oltre, eventi,, cors, workshop,, conc, filon, fier, storic, light, sound, live,, scen, palc, padiglion, allest, soluzion, sponsor, professional, vide, broadcast, music, producer, ovvi, integrated, systems, svilupp, collabor, connession, fianc, epson, screenlin, connession, progett, allest, tecnic, svilupp, experienc, pens, proporr, utent, esperit, real, opportunit, offert, integr, prodott, tecnolog, experienc, allest, soluzioni,, integr, hoc,, sponsor,, teatr, seminar, incontri,, giorno,, fier, format, aperto,, frequent, libera,, funzion, visit, combin, agevol, seminar, fiera,, osserv, contemp, soluzion, funzion, integr, guidate,, men,, seminari,, inoltr, accredit, press, ordin, collegi,, vist, presenz, utent, final, dover, ringraz, partecip, sosten, iniz, screenline,, home,, dodicifacce,, christ, assoc, contribu, avix, sdvo, entramb, protagon, intervent, prossim, report, rest, darv, appunt, prossim]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# reviews = data.map(lambda x : x['Review Text']).filter(lambda x: x is not None)\n",
    "# StopWords = stopwords.words(\"english\")\n",
    "# tokens = reviews                                                   \\\n",
    "#     .map( lambda document: document.strip().lower())               \\\n",
    "#     .map( lambda document: re.split(\" \", document))          \\\n",
    "#     .map( lambda word: [x for x in word if x.isalpha()])           \\\n",
    "#     .map( lambda word: [x for x in word if len(x) > 3] )           \\\n",
    "#     .map( lambda word: [x for x in word if x not in StopWords])    \\\n",
    "#     .zipWithIndex()\n",
    "\n",
    "clean_text.select(\"final\").show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def stem(in_vec):\n",
    "#     out_vec = []\n",
    "#     for t in in_vec:\n",
    "#         t_stem = stemmer.stem(t)\n",
    "#         if len(t_stem) > 2:\n",
    "#             out_vec.append(t_stem)       \n",
    "#     return out_vec\n",
    "\n",
    "# # Create user defined function for stemming with return type Array<String>\n",
    "# from pyspark.sql.types import *\n",
    "# stemmer_udf = udf(lambda x: stem(x), ArrayType(StringType()))\n",
    "# # Create new df with vectors containing the stemmed tokens \n",
    "# wordsDataFrame = (\n",
    "#     wordsDataFrame\n",
    "#         .withColumn(\"vector_stemmed\", stemmer_udf(\"filtered\"))\n",
    "#         .select(\"vector_stemmed\")\n",
    "#   )\n",
    "\n",
    "# # Rename df and column for clarity\n",
    "# production_df = wordsDataFrame.select(col(\"vector_stemmed\").alias(\"unigrams\"))\n",
    "\n",
    "# # Display\n",
    "# production_df.printSchema()\n",
    "# production_df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dividiamo il dataset in train e test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               final|\n",
      "+--------------------+\n",
      "|                  []|\n",
      "|[accant, sardegn,...|\n",
      "|[acciaier, venete...|\n",
      "|[adozion, distanz...|\n",
      "|[aere, scompars, ...|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+--------------------+\n",
      "|               final|\n",
      "+--------------------+\n",
      "|[acqua, econom, b...|\n",
      "|[agcom, estend, a...|\n",
      "|[aggrav, bilanc, ...|\n",
      "|[aics, casert, vi...|\n",
      "|[alasd, macleod, ...|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train, test = clean_text.select(\"final\").randomSplit([0.75, 0.25])\n",
    "train.show(5)\n",
    "test.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train = train.na.drop(how=\"all\")\n",
    "#train = train.drop(train.take(1))\n",
    "#train.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creiamo la TF-IDF matrix\n",
    "\n",
    "- tf: data da CountVectorizer\n",
    "- idf: data da IDF, che prende in input il dataset ricavato dal CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|               final|         rawFeatures|\n",
      "+--------------------+--------------------+\n",
      "|                  []|        (3322,[],[])|\n",
      "|[accant, sardegn,...|(3322,[6,8,12,13,...|\n",
      "|[acciaier, venete...|(3322,[0,2,4,8,22...|\n",
      "|[adozion, distanz...|(3322,[0,1,2,3,4,...|\n",
      "|[aere, scompars, ...|(3322,[2,5,11,12,...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#clean_text.select(\"final\").show(3)\n",
    "#cv = CountVectorizer(inputCol=\"final\", outputCol=\"rawFeatures\", vocabSize = 4000, minDF = 5)\n",
    "#cvmodel = cv.fit(clean_text)\n",
    "#featurizedData = cvmodel.transform(clean_text)\n",
    "#featurizedData.select(\"final\",\"rawFeatures\").show(5)\n",
    "cv_train = CountVectorizer(inputCol=\"final\", outputCol=\"rawFeatures\", vocabSize = 4000, minDF = 5)\n",
    "cvmodel = cv_train.fit(train)\n",
    "cvDatasetTrain = cvmodel.transform(train)\n",
    "cvDatasetTrain.select(\"final\",\"rawFeatures\").show(5)\n",
    "cvDatasetTest = cvmodel.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = cvmodel.vocabulary\n",
    "vocab_broadcast = spark.sparkContext.broadcast(vocab)\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "#idfModel = idf.fit(featurizedData)\n",
    "#rescaledData = idfModel.transform(featurizedData)\n",
    "idfModel = idf.fit(cvDatasetTrain)\n",
    "rescaledDataTrain = idfModel.transform(cvDatasetTrain)\n",
    "rescaledDataTest = idfModel.transform(cvDatasetTest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rescaledData.select(\"features\").show(2, False)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modello"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+\n",
      "|topic|         termIndices|         termWeights|\n",
      "+-----+--------------------+--------------------+\n",
      "|    0|[1738, 374, 1207,...|[0.03053298034404...|\n",
      "|    1|[912, 720, 1711, ...|[0.03944533340000...|\n",
      "|    2|[313, 429, 891, 1...|[0.05238150389240...|\n",
      "|    3|[171, 76, 277, 20...|[0.00773294918387...|\n",
      "|    4|[261, 294, 253, 4...|[0.07005991460847...|\n",
      "|    5|[967, 763, 418, 1...|[0.02425414231040...|\n",
      "|    6|[318, 630, 159, 1...|[0.03368926436323...|\n",
      "|    7|[943, 443, 197, 8...|[0.01112690965654...|\n",
      "|    8|[82, 147, 122, 59...|[0.00774487301626...|\n",
      "|    9|[327, 126, 121, 3...|[0.01216219696826...|\n",
      "|   10|[1137, 1383, 1531...|[0.04610724646007...|\n",
      "|   11|[280, 507, 22, 29...|[0.00796826725430...|\n",
      "|   12|[239, 27, 190, 48...|[0.01692770559317...|\n",
      "|   13|[284, 30, 454, 14...|[0.00804463792291...|\n",
      "|   14|[12, 131, 163, 15...|[0.00539198014365...|\n",
      "+-----+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Num_Argomenti = [10, 20, 50, 100, 150] #GRID SEARCH per decidere il numero di topic k\n",
    "# loglik=[]\n",
    "# logperp=[]\n",
    "# n_iter = {Num_Argomenti[0]: 20, Num_Argomenti[1]: 40, Num_Argomenti[2]: 100, Num_Argomenti[3]:180, Num_Argomenti[4]: 260}\n",
    "# for i in Num_Argomenti:\n",
    "#     print(i)\n",
    "#   http://localhost:8888/notebooks/Desktop/Materiale%20didattico/LDA/MIO_ULTIMO.ipynb#  lda = LDA(k=i, seed=123, optimizer=\"online\", featuresCol=\"features\", optimizeDocConcentration= True, maxIter= n_iter[i])\n",
    "#     ldamodel = lda.fit(rescaledData)\n",
    "#     loglik.append(ldamodel.logLikelihood(rescaledData))\n",
    "#     logperp.append(ldamodel.logPerplexity(rescaledData))\n",
    "#ldanew = LDA(k=15, seed=123, optimizer=\"online\",featuresCol=\"features\", subsamplingRate = 0.2,maxIter = 20)# optimizeDocConcentration= True, subsamplingRate = 0.2, learningDecay = 0.5, \n",
    "#ldamodelNEW = ldanew.fit(rescaledDataTrain)\n",
    "k = 15\n",
    "lda = LDA(k=k, seed=123, optimizer=\"online\",featuresCol=\"features\", maxIter = 2000 )# optimizeDocConcentration= True, subsamplingRate = 0.2, learningDecay = 0.5, \n",
    "ldamodel = lda.fit(rescaledDataTrain)\n",
    "ldatopics = ldamodel.describeTopics()\n",
    "ldatopics.show(k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = ldamodel.transform(rescaledDataTest) #previsioni sul test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o1052.save.\n: org.apache.spark.SparkException: Job aborted.\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:96)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1096)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1094)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1094)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1094)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply$mcV$sp(PairRDDFunctions.scala:1067)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1032)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1032)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1032)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply$mcV$sp(PairRDDFunctions.scala:958)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:958)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:958)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:957)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply$mcV$sp(RDD.scala:1493)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1472)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1472)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\r\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1472)\r\n\tat org.apache.spark.ml.util.DefaultParamsWriter$.saveMetadata(ReadWrite.scala:283)\r\n\tat org.apache.spark.ml.clustering.LocalLDAModel$LocalLDAModelWriter.saveImpl(LDA.scala:605)\r\n\tat org.apache.spark.ml.util.MLWriter.save(ReadWrite.scala:103)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 4042.0 failed 1 times, most recent failure: Lost task 0.0 in stage 4042.0 (TID 4045, localhost, executor driver): java.io.IOException: (null) entry in command string: null chmod 0644 C:\\Users\\paperspace\\Desktop\\LDA\\lda\\metadata\\_temporary\\0\\_temporary\\attempt_20180709070853_6245_m_000000_0\\part-00000\r\n\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:770)\r\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:866)\r\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:849)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:733)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:225)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:398)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)\r\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)\r\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:804)\r\n\tat org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:123)\r\n\tat org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.initWriter(SparkHadoopWriter.scala:224)\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.org$apache$spark$internal$io$SparkHadoopWriter$$executeTask(SparkHadoopWriter.scala:118)\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:79)\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:78)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1599)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1587)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1586)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1586)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1820)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1769)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1758)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2027)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2048)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2080)\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:78)\r\n\t... 42 more\r\nCaused by: java.io.IOException: (null) entry in command string: null chmod 0644 C:\\Users\\paperspace\\Desktop\\LDA\\lda\\metadata\\_temporary\\0\\_temporary\\attempt_20180709070853_6245_m_000000_0\\part-00000\r\n\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:770)\r\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:866)\r\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:849)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:733)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:225)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:398)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)\r\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)\r\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:804)\r\n\tat org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:123)\r\n\tat org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.initWriter(SparkHadoopWriter.scala:224)\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.org$apache$spark$internal$io$SparkHadoopWriter$$executeTask(SparkHadoopWriter.scala:118)\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:79)\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:78)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\t... 1 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-71-f72113b1bf7b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#prediction.select(\"topicDistribution\").show(2, False)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mlocal_model_path\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;34m\"C:/Users/paperspace/Desktop/LDA/lda\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mldamodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlocal_model_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;31m#provando = LDA.load(\"/lda2000it\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#spark.sparkContext.getLocalProperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pyspark\\ml\\util.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, path)\u001b[0m\n\u001b[0;32m    202\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m         \u001b[1;34m\"\"\"Save this ML instance to the given path, a shortcut of 'write().save(path)'.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 204\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    205\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pyspark\\ml\\util.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, path)\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbasestring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    164\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"path should be a basestring, got type %s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 165\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    166\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    167\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1158\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1159\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1160\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1161\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1162\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    318\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    319\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 320\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    321\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    322\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o1052.save.\n: org.apache.spark.SparkException: Job aborted.\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:96)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1096)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1094)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1094)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1094)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply$mcV$sp(PairRDDFunctions.scala:1067)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1032)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1032)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1032)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply$mcV$sp(PairRDDFunctions.scala:958)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:958)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:958)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:957)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply$mcV$sp(RDD.scala:1493)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1472)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1472)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\r\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1472)\r\n\tat org.apache.spark.ml.util.DefaultParamsWriter$.saveMetadata(ReadWrite.scala:283)\r\n\tat org.apache.spark.ml.clustering.LocalLDAModel$LocalLDAModelWriter.saveImpl(LDA.scala:605)\r\n\tat org.apache.spark.ml.util.MLWriter.save(ReadWrite.scala:103)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 4042.0 failed 1 times, most recent failure: Lost task 0.0 in stage 4042.0 (TID 4045, localhost, executor driver): java.io.IOException: (null) entry in command string: null chmod 0644 C:\\Users\\paperspace\\Desktop\\LDA\\lda\\metadata\\_temporary\\0\\_temporary\\attempt_20180709070853_6245_m_000000_0\\part-00000\r\n\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:770)\r\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:866)\r\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:849)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:733)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:225)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:398)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)\r\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)\r\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:804)\r\n\tat org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:123)\r\n\tat org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.initWriter(SparkHadoopWriter.scala:224)\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.org$apache$spark$internal$io$SparkHadoopWriter$$executeTask(SparkHadoopWriter.scala:118)\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:79)\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:78)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1599)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1587)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1586)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1586)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1820)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1769)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1758)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2027)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2048)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2080)\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:78)\r\n\t... 42 more\r\nCaused by: java.io.IOException: (null) entry in command string: null chmod 0644 C:\\Users\\paperspace\\Desktop\\LDA\\lda\\metadata\\_temporary\\0\\_temporary\\attempt_20180709070853_6245_m_000000_0\\part-00000\r\n\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:770)\r\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:866)\r\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:849)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:733)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:225)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:398)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)\r\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)\r\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:804)\r\n\tat org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:123)\r\n\tat org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.initWriter(SparkHadoopWriter.scala:224)\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.org$apache$spark$internal$io$SparkHadoopWriter$$executeTask(SparkHadoopWriter.scala:118)\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:79)\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:78)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\t... 1 more\r\n"
     ]
    }
   ],
   "source": [
    "#prediction.select(\"topicDistribution\").show(2, False)\n",
    "local_model_path =\"C:/Users/paperspace/Desktop/LDA/lda\"\n",
    "ldamodel.save(local_model_path)\n",
    "#provando = LDA.load(\"/lda2000it\")\n",
    "#spark.sparkContext.getLocalProperty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train-set results\n",
      "LogLikelihood -1930441.43 LogPerplexity 7.44\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(\"Train-set results\")\n",
    "print(\"LogLikelihood\", np.round(ldamodel.logLikelihood(rescaledDataTrain),2), \"LogPerplexity\", np.round(ldamodel.logPerplexity(rescaledDataTrain),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test-set results\n",
      "LogLikelihood -718901.52 LogPerplexity 8.05\n"
     ]
    }
   ],
   "source": [
    "# import numpy as np\n",
    "# plt.plot(Num_Argomenti, loglik)\n",
    "# print(logperp)\n",
    "# print(loglik)\n",
    "print(\"Test-set results\")\n",
    "print(\"LogLikelihood\", np.round(ldamodel.logLikelihood(rescaledDataTest),2), \"LogPerplexity\", np.round(ldamodel.logPerplexity(rescaledDataTest),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic description\n",
    "\n",
    "Per ogni topic abbiamo un insieme di parole che lo descrivono, io ho scelto di memorizzare le prime 10 e mostrarle a schermo in modo da poter capire di che topic si tratta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+\n",
      "|topic|         termIndices|         termWeights|\n",
      "+-----+--------------------+--------------------+\n",
      "|    0|[1738, 374, 1207,...|[0.03053298034404...|\n",
      "|    1|[912, 720, 1711, ...|[0.03944533340000...|\n",
      "|    2|[313, 429, 891, 1...|[0.05238150389240...|\n",
      "|    3|[171, 76, 277, 20...|[0.00773294918387...|\n",
      "|    4|[261, 294, 253, 4...|[0.07005991460847...|\n",
      "|    5|[967, 763, 418, 1...|[0.02425414231040...|\n",
      "|    6|[318, 630, 159, 1...|[0.03368926436323...|\n",
      "|    7|[943, 443, 197, 8...|[0.01112690965654...|\n",
      "|    8|[82, 147, 122, 59...|[0.00774487301626...|\n",
      "|    9|[327, 126, 121, 3...|[0.01216219696826...|\n",
      "|   10|[1137, 1383, 1531...|[0.04610724646007...|\n",
      "|   11|[280, 507, 22, 29...|[0.00796826725430...|\n",
      "|   12|[239, 27, 190, 48...|[0.01692770559317...|\n",
      "|   13|[284, 30, 454, 14...|[0.00804463792291...|\n",
      "|   14|[12, 131, 163, 15...|[0.00539198014365...|\n",
      "+-----+--------------------+--------------------+\n",
      "\n",
      "+-----+-----------------------------------------------------------------------------------------+\n",
      "|topic|topic_desc                                                                               |\n",
      "+-----+-----------------------------------------------------------------------------------------+\n",
      "|0    |[linguist, vittor, scompars, spost, albert, ragazz, silv, mercoled, magg, tapp]          |\n",
      "|1    |[ghiacc, design, vettur, campion, sport, classif, stradal, leader, sponsor, principal]   |\n",
      "|2    |[capell, estat, ideal, hotel, lisc, color, bagn, abbin, moss, perfett]                   |\n",
      "|3    |[digital, ricerc, process, impres, attivit, tecnolog, svilupp, progett, serviz, business]|\n",
      "|4    |[localit, racchett, profil, spin, ibrid, bresc, iscrizion, giocator, cord, elenc]        |\n",
      "|5    |[mobilit, ceram, circol, spiagg, allag, vial, veicol, ansa, tren, sardegn]               |\n",
      "|6    |[fotograf, matrimon, francesc, adozion, municip, alleg, spos, relig, profond, document]  |\n",
      "|7    |[catan, poliz, controll, indagin, carabinier, carc, agent, arrest, rapin, incident]      |\n",
      "|8    |[artist, festival, teatr, piazz, edizion, citt, arte, event, cultural, spettacol]        |\n",
      "|9    |[farmac, acquist, milion, vend, merc, cost, miliard, prezz, samsung, offert]             |\n",
      "|10   |[spagn, marocc, iran, mediaset, pension, protett, salvaguard, sentenz, ombra, finalit]   |\n",
      "|11   |[russ, brasil, parl, giornal, compagn, govern, sold, turc, violenz, scarp]               |\n",
      "|12   |[squadr, gioc, sport, allen, calc, milan, sogn, torne, giorg, giocator]                  |\n",
      "|13   |[cook, polit, trump, inform, sindac, cittadin, amministr, consigl, cookies, comunal]     |\n",
      "|14   |[trov, sant, padr, chies, vers, ogni, testimon, poss, stagion, immagin]                  |\n",
      "+-----+-----------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ldatopics = ldamodel.describeTopics()\n",
    "ldatopics.show(150)\n",
    "def map_termID_to_Word(termIndices):\n",
    "    words = []\n",
    "    for termID in termIndices:\n",
    "        words.append(vocab_broadcast.value[termID])\n",
    "    \n",
    "    return words\n",
    "\n",
    "udf_map_termID_to_Word = udf(map_termID_to_Word , ArrayType(StringType()))\n",
    "ldatopics_mapped = ldatopics.withColumn(\"topic_desc\", udf_map_termID_to_Word(ldatopics.termIndices))\n",
    "ldaResults = ldamodel.transform(rescaledDataTrain)\n",
    "\n",
    "ldatopics_mapped.select(ldatopics_mapped.topic, ldatopics_mapped.topic_desc).show(150,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def breakout_array(index_number, record):\n",
    "    vectorlist = record.tolist()\n",
    "    return vectorlist[index_number]\n",
    "\n",
    "udf_breakout_array = udf(breakout_array, FloatType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In questo modo possiamo mostrare per ogni documento la percentuale di appartenenza ad un certo topics   \n",
    "\n",
    "\n",
    "Da sistemare\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# #enrichedData = ldaResults.withColumn(\"Topic_0\", udf_breakout_array(lit(0), ldaResults.topicDistribution)).withColumn(\"Topic_1\", udf_breakout_array(lit(1), ldaResults.topicDistribution)).withColumn(\"Topic_2\", udf_breakout_array(lit(2), ldaResults.topicDistribution)).withColumn(\"Topic_3\", udf_breakout_array(lit(3), ldaResults.topicDistribution)).withColumn(\"Topic_4\", udf_breakout_array(lit(4), ldaResults.topicDistribution)).withColumn(\"Topic_5\", udf_breakout_array(lit(5), ldaResults.topicDistribution)).withColumn(\"Topic_6\", udf_breakout_array(lit(6), ldaResults.topicDistribution)).withColumn(\"Topic_7\", udf_breakout_array(lit(7), ldaResults.topicDistribution)).withColumn(\"Topic_8\", udf_breakout_array(lit(8), ldaResults.topicDistribution)).withColumn(\"Topic_9\", udf_breakout_array(lit(9), ldaResults.topicDistribution)).withColumn(\"Topic_10\", udf_breakout_array(lit(10), ldaResults.topicDistribution)).withColumn(\"Topic_11\", udf_breakout_array(lit(11), ldaResults.topicDistribution)).withColumn(\"Topic_12\", udf_breakout_array(lit(12), ldaResults.topicDistribution)).withColumn(\"Topic_13\", udf_breakout_array(lit(13), ldaResults.topicDistribution)).withColumn(\"Topic_14\", udf_breakout_array(lit(14), ldaResults.topicDistribution)).withColumn(\"Topic_15\", udf_breakout_array(lit(15), ldaResults.topicDistribution))    \n",
    "\n",
    "# enrichedData = prediction \\\n",
    "#         .withColumn(\"Topic_0\", udf_breakout_array(lit(0), prediction.topicDistribution))    \\\n",
    "#         .withColumn(\"Topic_1\", udf_breakout_array(lit(1), prediction.topicDistribution))    \\\n",
    "#         .withColumn(\"Topic_2\", udf_breakout_array(lit(2), prediction.topicDistribution))    \\\n",
    "#         .withColumn(\"Topic_3\", udf_breakout_array(lit(3), prediction.topicDistribution))    \\\n",
    "#         .withColumn(\"Topic_4\", udf_breakout_array(lit(4), prediction.topicDistribution))    \\\n",
    "#         .withColumn(\"Topic_5\", udf_breakout_array(lit(5), prediction.topicDistribution))    \\\n",
    "#         .withColumn(\"Topic_6\", udf_breakout_array(lit(6), prediction.topicDistribution))    \\\n",
    "#         .withColumn(\"Topic_7\", udf_breakout_array(lit(7), prediction.topicDistribution))    \\\n",
    "#         .withColumn(\"Topic_8\", udf_breakout_array(lit(8), prediction.topicDistribution))    \\\n",
    "#         .withColumn(\"Topic_9\", udf_breakout_array(lit(9), prediction.topicDistribution))    \n",
    "#         .withColumn(\"Topic_10\", udf_breakout_array(lit(10), prediction.topicDistribution))  \\\n",
    "#         .withColumn(\"Topic_11\", udf_breakout_array(lit(11), prediction.topicDistribution))  \\\n",
    "#         .withColumn(\"Topic_12\", udf_breakout_array(lit(12), prediction.topicDistribution))  \\\n",
    "#         .withColumn(\"Topic_13\", udf_breakout_array(lit(13), prediction.topicDistribution))  \\\n",
    "#         .withColumn(\"Topic_14\", udf_breakout_array(lit(14), prediction.topicDistribution))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+-------------+------------+------------+------------+------------+\n",
      "|               final|         rawFeatures|            features|   topicDistribution|     Topic_10|    Topic_11|    Topic_12|    Topic_13|    Topic_14|\n",
      "+--------------------+--------------------+--------------------+--------------------+-------------+------------+------------+------------+------------+\n",
      "|[acqua, econom, b...|(3322,[0,1,2,3,6,...|(3322,[0,1,2,3,6,...|[3.78855692362897...| 0.0054824986| 0.076581135|  0.02965965| 0.047862526| 0.062499925|\n",
      "|[agcom, estend, a...|(3322,[0,1,12,13,...|(3322,[0,1,12,13,...|[7.82727893993767...|  0.017529093| 2.413132E-4|1.9457372E-4|  0.22001347|  0.09607001|\n",
      "|[aggrav, bilanc, ...|(3322,[0,1,5,6,7,...|(3322,[0,1,5,6,7,...|[3.72983733691397...| 2.4193954E-5|    0.535461| 0.014609428| 0.100260206| 0.028937029|\n",
      "|[aics, casert, vi...|(3322,[5,7,11,15,...|(3322,[5,7,11,15,...|[0.01964151795991...| 9.2568895E-5|  0.29339018|  0.34435052|4.9734954E-4|  5.32602E-4|\n",
      "|[alasd, macleod, ...|(3322,[1,2,3,4,5,...|(3322,[1,2,3,4,5,...|[1.16736390237408...| 0.0027148535|  0.08357234| 2.902026E-5|  0.07067756| 0.077709995|\n",
      "|[aless, balsam, g...|(3322,[0,1,4,5,9,...|(3322,[0,1,4,5,9,...|[4.95209896067632...| 3.2122272E-5|  0.13613851|   0.1423314| 1.727274E-4|   0.2997349|\n",
      "|[allarm, simultan...|(3322,[5,6,7,11,1...|(3322,[5,6,7,11,1...|[4.92592637895544...|  0.028895374|  0.31775814|1.2246025E-4| 0.071758315| 0.090427525|\n",
      "|[ancor, lui,, lui...|(3322,[7,8,11,16,...|(3322,[7,8,11,16,...|[0.03740739962985...|  8.484051E-5|   0.3655283|  0.04836424|  0.12107101|  0.26774427|\n",
      "|[ansa, catania,, ...|(3322,[0,16,20,74...|(3322,[0,16,20,74...|[2.22654066454232...| 1.4442674E-4|6.8630173E-4|5.5347616E-4| 7.755995E-4|8.2972593E-4|\n",
      "|[ansa, istanbul,,...|(3322,[6,10,11,29...|(3322,[6,10,11,29...|[3.12942596040081...| 2.0299327E-4|    0.494408| 7.779288E-4|  0.22015339|0.0011667389|\n",
      "|[anton, candr, in...|(3322,[7,8,25,44,...|(3322,[7,8,25,44,...|[1.68332024736785...| 1.0919021E-4| 5.191852E-4|  0.30354473|  0.14400752|  0.19679964|\n",
      "|[archiv, notiz, a...|(3322,[2,4,10,20,...|(3322,[2,4,10,20,...|[1.78978523608387...| 1.1609617E-4|  0.13568455|4.4492248E-4|  0.09839772|  0.09585026|\n",
      "|[archiv, notiz, a...|(3322,[0,2,3,4,6,...|(3322,[0,2,3,4,6,...|[0.01170771196206...|  4.260911E-5|2.0255974E-4| 0.062283184|2.2913051E-4|  0.07853599|\n",
      "|[arco, comit, reg...|(3322,[0,2,7,9,10...|(3322,[0,2,7,9,10...|[0.04261608865302...| 2.2868297E-5|  0.12924272|  0.26070246|  0.03547229| 0.010889432|\n",
      "|[arezz, piccol, p...|(3322,[0,3,6,8,11...|(3322,[0,3,6,8,11...|[9.56510700629795...|    6.2045E-5|  0.10402228|  0.05742421| 0.030103654|  0.13155894|\n",
      "|[ariann, felic, s...|(3322,[1,3,5,7,9,...|(3322,[1,3,5,7,9,...|[8.26305407156972...| 5.3599106E-5| 0.071231596|  0.03757642|    0.043231|  0.34517592|\n",
      "|[artiac, traged, ...|(3322,[2,8,10,12,...|(3322,[2,8,10,12,...|[1.37823199589552...|  0.022106253|4.2506732E-4|  0.08359249| 4.806186E-4|  0.49288985|\n",
      "|[assegn, familiar...|(3322,[1,2,11,13,...|(3322,[1,2,11,13,...|[8.04440199392948...|  0.020082278|2.4810468E-4|1.9996891E-4| 0.079030186| 0.092584126|\n",
      "|[attualit, ternen...|(3322,[7,26,28,63...|(3322,[7,26,28,63...|[1.63543839919054...|1.06084306E-4| 5.043267E-4| 0.109090425|  0.45882776|6.1041344E-4|\n",
      "|[aument, famigl, ...|(3322,[3,6,7,8,12...|(3322,[3,6,7,8,12...|[4.00287727359307...|  0.004276616| 1.234471E-4| 9.950835E-5|  0.04309333|  0.24963701|\n",
      "+--------------------+--------------------+--------------------+--------------------+-------------+------------+------------+------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# enrichedData.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
