{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m pip install --upgrade pip\n",
    "# !pip install msgpack\n",
    "# !pip install ipython\n",
    "# !pip install pyspark==2.3.0\n",
    "# !pip install pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# os.environ[\"JAVA_HOME\"] = \"C:\\Java\\jdk1.8.0_172\"\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = \"--master local[2] pyspark-shell\"\n",
    "os.environ[\"JAVA_HOME\"] = \"C:/jdk1.8.0_171\"\n",
    "#USATO PER RISOLVEREE PROBLEMA JAVA GATEWAY ..... (ho anche installato java e inserito manualmente java home tra le variabili ambiente)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Roberta\n",
      "[nltk_data]     PC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer, CountVectorizer\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.linalg import Vectors, SparseVector\n",
    "from pyspark.ml.clustering import LDA, BisectingKMeans\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import pyspark\n",
    "import string\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.mllib.util import MLUtils\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.feature import CountVectorizer, CountVectorizerModel, Tokenizer, RegexTokenizer, StopWordsRemover\n",
    "nltk.download(\"stopwords\")\n",
    "import pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('NLP_topicModel').getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read_data_from_json(filename, multiline = True):\n",
    "    df = spark.read.json(filename, multiLine = multiline)\n",
    "\n",
    "    return df\n",
    "def read_our_json_dataset (filename, multiline = True):\n",
    "    df = read_data_from_json(filename, multiline)\n",
    "    df = df.fillna({'testo_txt_it': ''})\n",
    "    df = df.withColumn(\"uid\", monotonically_increasing_id())\n",
    "    df = df.withColumn(\"year_month\", df.published_dt.substr(1,7))\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myStemmer (record, array=False):\n",
    "    stemmer = SnowballStemmer(\"italian\")\n",
    "    if array:\n",
    "        text_out = [stemmer.stem(word) for word in record] \n",
    "    else:    \n",
    "        text = record [2]#9\n",
    "        text_out = [stemmer.stem(word) for word in text]\n",
    "    return(text_out)\n",
    "\n",
    "# def myLemmatizer (record):\n",
    "#     tagger = ttw.TreeTagger(TAGLANG='it')\n",
    "#     if array:\n",
    "#         tags = tagger.tag_text(record)  \n",
    "#         tags2 = ttw.make_tags(tags)\n",
    "#         final_tags = [tags2[i][2] for i in range (len(tags2))]\n",
    "#     else:\n",
    "#         text = record[2]\n",
    "#         tags = tagger.tag_text(text)\n",
    "#         tags2=ttw.make_tags(tags)\n",
    "#         final_tags = [tags2[i][2] for i in range (len(tags2))]\n",
    "#     return(final_tags)\n",
    "\n",
    "def removePunctuation(column):\n",
    "    return trim(lower(regexp_replace(column, '[^A-Za-z]', ' '))).alias('sentence')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_token(df, input_col = \"sentence\", output_col = \"tokenized\"):\n",
    "    tokenizer = Tokenizer(inputCol = input_col, outputCol = output_col)#testo_txt_it\n",
    "    wordsDataFrame = tokenizer.transform(df)#rawdata\n",
    "    return(wordsDataFrame)\n",
    "\n",
    "def get_stemmed_words (df,input_col = \"tokenized\", output_col = \"stemmed\"):\n",
    "    remover = StopWordsRemover(inputCol = input_col , outputCol=\"tokenizedNew\", stopWords = [\" \", \"\"])\n",
    "    df = remover.transform(df)\n",
    "    udf_myStemmer = udf(myStemmer, (ArrayType(StringType()))) # if the function returns an int\n",
    "    df = df.withColumn(output_col, udf_myStemmer(struct([df[x] for x in df.columns])))\n",
    "    return (df)\n",
    "\n",
    "def words_widely_used_and_short (df, input_col = \"stemmed\", number_of_words = 100):\n",
    "    cv_tmp = CountVectorizer(inputCol = input_col, outputCol=\"tmp_vectors\")\n",
    "    cv_tmp_model = cv_tmp.fit(df)\n",
    "    top_words = list(cv_tmp_model.vocabulary[0:number_of_words])\n",
    "    less_then_3_charachters = [word for word in cv_tmp_model.vocabulary if len(word) <= 3 ]\n",
    "    return(top_words , less_then_3_charachters)\n",
    "\n",
    "def collect_stopwords (df, input_col = \"stemmed\", number_of_words = 100):\n",
    "    top_words, less_then_3_charachters = words_widely_used_and_short(df, input_col, number_of_words)\n",
    "    stopWordsNLTK = list(set(stopwords.words('english')))+list(set(stopwords.words('italian')))\n",
    "    stopWordsCustom = [\" \",\"\", \"dal\", \"al\",\"davan\",\"avev\",\"qualc\", \"qualcuno\", \"qualcosa\", \"avevano\", \"davanti\", \"aveva\",\"e\",\"avere\", \"fare\",\"la\",\"li\", \"lo\", \"gli\", \"essere\", \"solo\", \"per\", \"cosa\", \"ieri\",\"disponibile\", \"anno\", \"detto\", \"quando\",\"fatto\", \"sotto\", \"alcuna\", \"quali\"]\n",
    "#Add additional stopwords in th, is list\n",
    "    stopWordsPySpark = StopWordsRemover.loadDefaultStopWords(\"italian\")\n",
    "#Combine all the stopwords\n",
    "    stpw = top_words + stopWordsNLTK  + stopWordsCustom +stopWordsPySpark+ less_then_3_charachters \n",
    "    stem_stopw = myStemmer(stpw, True) #stemming the stopwords\n",
    "    return (stpw+stem_stopw)\n",
    "\n",
    "def remove_stopwords_train (df, input_col = \"stemmed\", output_col = \"final\", number_of_words = 100):\n",
    "    stopwords = collect_stopwords(df, input_col, number_of_words )\n",
    "    removerNew = StopWordsRemover(inputCol = input_col, outputCol = output_col, stopWords = stopwords) #Remove stopwords from the tokenized list\n",
    "    new_df= removerNew.transform(df)#dropping the stemmed stopwords from the stemmed word \n",
    "    return(new_df, stopwords)\n",
    "\n",
    "\n",
    "def remove_stopwords_test (df, stopwords, input_col = \"stemmed\", output_col = \"final\"):\n",
    "    removerNew = StopWordsRemover(inputCol = input_col, outputCol = output_col, stopWords = stopwords) #Remove stopwords from the tokenized list\n",
    "    new_df= removerNew.transform(df)#dropping the stemmed stopwords from the stemmed word \n",
    "    return(new_df)\n",
    "    \n",
    "def check_and_remove_null_string (df):\n",
    "    pandasdf = df.toPandas()\n",
    "    indexes = [i for i in range (len (pandasdf)) if pandasdf.iloc[i][0] == []]\n",
    "    if len(indexes)!= 0:\n",
    "        dfnew = spark.createDataFrame(pandasdf.drop(index = indexes ))\n",
    "        return(dfnew)\n",
    "    else: \n",
    "        return (df)\n",
    "    \n",
    "def pulizia_df_train (df, text_col = \"testo_txt_it\", number_of_words =100):\n",
    "    pulito = df.select(removePunctuation(col(text_col), \"sentence\"))\n",
    "#Tokenize the text in the text column\n",
    "    wordsDataFrame = get_token(pulito,\"sentence\",\"tokenized\" )    \n",
    "    wordsDataFrame = get_stemmed_words(wordsDataFrame, \"tokenized\", \"stemmed\")\n",
    "    df_clean, stopwords = remove_stopwords_train (wordsDataFrame, \"stemmed\", \"final\", number_of_words)\n",
    "    clean_text = check_and_remove_null_string(df_clean)\n",
    "    return(clean_text, stopwords)\n",
    " \n",
    "def clean_test (df, stopwords, text_col = \"testo_txt_it\"):\n",
    "    pulito = df.select(removePunctuation(col(text_col), \"sentence\"))\n",
    "    wordsDataFrame = get_token(pulito,\"sentence\",\"tokenized\" )    \n",
    "    wordsDataFrame = get_stemmed_words(wordsDataFrame, \"tokenized\", \"stemmed\")\n",
    "    removerNew = StopWordsRemover(inputCol = \"stemmed\", outputCol = \"final\", stopWords = stopwords) #Remove stopwords from the tokenized list\n",
    "    new_df2= removerNew.transform(wordsDataFrame)\n",
    "    clean_text = check_and_remove_null_string(new_df2)\n",
    "    return(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_test (df, perc_train = 0.95, perc_test = 0.05):\n",
    "    train, test = df.select(\"final\").randomSplit([perc_train, perc_test])\n",
    "    return (train, test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def tf_train (df):\n",
    "    cv_train = CountVectorizer(inputCol=\"final\", outputCol=\"rawFeatures\", vocabSize = 4000, minDF = 3, minTF = 2)\n",
    "    cvmodel = cv_train.fit(df)\n",
    "    cvDatasetTrain = cvmodel.transform(df)\n",
    "    return (cvmodel, cvDatasetTrain)\n",
    "\n",
    "def idf_train (cvDatasetTrain):\n",
    "    idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "    idfModel = idf.fit(cvDatasetTrain)\n",
    "    rescaledDataTrain = idfModel.transform(cvDatasetTrain)\n",
    "    return (idfModel, rescaledDataTrain)\n",
    "\n",
    "def tf_idf_train (df):\n",
    "    cvmodel, cvDatasetTrain = tf_train(df)\n",
    "    idfModel, rescaledDataTrain = idf_train(cvDatasetTrain)\n",
    "    return (cvmodel, cvDatasetTrain, idfModel, rescaledDataTrain)\n",
    "\n",
    "def tf_test (df, cvmodel):    \n",
    "    return (cvmodel.transform(df))\n",
    "\n",
    "def idf_test (idfModel, cvDatasetTest):\n",
    "    return (idfModel.transform(cvDatasetTest))\n",
    "\n",
    "def tf_idf_test (df, cvmodel, idfModel):\n",
    "    cvDatasetTest = tf_test (df, cvmodel)\n",
    "    rescaledDataTest = idf_test(idfModel, cvDatasetTest)\n",
    "    return (cvDatasetTest, rescaledDataTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "def doclengths_and_termfreq (countVectorizer_transf,column, df):\n",
    "    rows_contents = countVectorizer_transf.select(column).take(df.count())\n",
    "    doc_lengths=[]\n",
    "    term_frequencies_list = []\n",
    "    for i in range(len(rows_contents)):\n",
    "        sparse_vector = rows_contents[i].asDict()[column] #è uno sparse vector se printato visualizza solo elem nonzero e\n",
    "                                                    #rappresenta l'indice parola e la sua freq all'interno del documento\n",
    "        doc_lengths.append(sparse_vector.numNonzeros()) #numero elementi non zero all'interno dell'array (senza le parentesi da \n",
    "                                                        # l'indice degli elem nonzero)\n",
    "        term_frequencies_list.append(sparse_vector.toArray()) #toArray() mostra il vettore per intero, compresi gli zeri\n",
    "    term_frequency = list(np.sum(term_frequencies_list, axis = 0)) #andiamo a sommare i vettori dei vari documenti, per ottenere le freq nel corpus\n",
    "    to_drop = [i for i in range(len(doc_lengths)) if doc_lengths[i] == 0]       \n",
    "    doc_lengths = [doc_lengths[i] for i in range(len(doc_lengths)) if i not in to_drop]  \n",
    "    return(term_frequency, doc_lengths, to_drop)\n",
    "    \n",
    "def extract_data (ldamodel, countVectorizer_transf, countVectorizer_fit, df, results_pred, column = \"rawFeatures\"):\n",
    "    vocab = countVectorizer_fit.vocabulary\n",
    "    term_frequency, doc_lengths, to_drop = doclengths_and_termfreq (countVectorizer_transf, column, df)\n",
    "    word_dists_topic = np.asmatrix(ldamodel.topicsMatrix().toArray())\n",
    "    #qui abbiamo per ogni parola la distribuzione nei vari topic (1000, 10), noi vogliamo l'opposto (10, 1000)\n",
    "    #quindi andiamo a trasporre la matrice appena creata e a trasformarla in una lista di liste (formato desiderato)\n",
    "    topic_term_dists = word_dists_topic.transpose().tolist()\n",
    "    pred_top_dists_pd = results_pred.select(\"topicDistribution\").toPandas()\n",
    "    doc_topic_dists = [list(pred_top_dists_pd.iloc[i][0]) for i in range(results_pred.count())]\n",
    "    doc_topic_dists = [doc_topic_dists[i] for i in range(len(doc_topic_dists)) if i not in to_drop]       \n",
    "#   results_visual = {\"vocab\": vocab, 'doc_lengths': doc_lengths , 'term_frequency': term_frequency, 'doc_topic_dists': doc_topic_dists, 'topic_term_dists': topic_term_dists}\n",
    "    return(vocab, doc_lengths, term_frequency, topic_term_dists, doc_topic_dists)\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
